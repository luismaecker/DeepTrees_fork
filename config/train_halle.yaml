name: train-halle
data:
  rasters: /work/ka1176/shared_data/2024-ufz-deeptree/polygon-labelling/tiles  # directory with raster images
  masks: /work/ka1176/shared_data/2024-ufz-deeptree/polygon-labelling/masks # directory with masks
  outlines: /work/ka1176/shared_data/2024-ufz-deeptree/polygon-labelling/outlines # directory with outlines
  dist: /work/ka1176/shared_data/2024-ufz-deeptree/polygon-labelling/dist_trafo # directory with distance transforms
  width: 256             # Width and height of the cropped images returned by the data loader
  batchsize: 2          # dataloader batch size
  training_split: 0.8    # Train/val split
  train_indices: null    # Train set indices (pass explicitly)
  val_indices: null      # Validation set indices (pass explicitly)
  concatenate_ndvi: True # If True, concatenate NDVI
  red: 0                 # Index of Red channel
  nir: 3                 # Index of Infrared channel
  num_workers: 8         # Number of workers in dataloader
pretrained:
  # path to pretrained models
  path: /work/ka1176/shared_data/2024-ufz-deeptree/pretrained_models/tcd-20cm-RGBI-v1/
  model: null # Pass e.g. Unet-resnet18_epochs=209_lr=0.0001_width=224_bs=32_divby=255_custom_color_augs_k=0_jitted.pt 
  
model:
  _target_: treecrowndelineation.model.tcd_model.TreeCrownDelineationModel
  in_channels: 5         # Number of input channels / bands of the input image
  architecture: Unet     # segmentation model architecture
  backbone: resnet18     # backbone
  lr: 0.0003             # learning rate
  mask_loss_share: 0.5   # contribution of masks (vs outline) to overall loss.
  freeze_layers: False   # freeze layers (keep only segmentation head trainable)
logdir: ${hydra.run.dir} # this is where we find logs, trained models, predictions
model_save_path: null # TODO
model_name: unet-halle # short and memorable model name
seed: null               # Fix random seed 

callbacks:
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: "val/iou_mask" # name of the logged metric which determines when model is improving
    mode: "max" # "max" means higher metric value is better, can be also "min"
    patience: 6 # how many validation epochs of not improving until training stops
    min_delta: 0 # minimum change in the monitored metric needed to qualify as an improvement

  model_summary:
    _target_: lightning.pytorch.callbacks.RichModelSummary
    max_depth: -1

  rich_progress_bar:
    _target_: lightning.pytorch.callbacks.RichProgressBar

  learning_rate_monitor:
    _target_: lightning.pytorch.callbacks.LearningRateMonitor

  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: "val/iou_mask" # name of the logged metric which determines when model is improving
    mode: "max" # "max" means higher metric value is better, can be also "min"
    save_top_k: 1 # save k best models (determined by above metric)
    save_last: True # additionaly always save model from last epoch
    verbose: False
    dirpath: "checkpoints/"
    filename: "epoch_{epoch:03d}"
    auto_insert_metric_name: False
    every_n_epochs: 1


trainer:
  _target_: lightning.Trainer
  devices: 1               # Number of devices (GPUs) to use
  accelerator: auto        # Choose GPU if available
  max_epochs: 100          # maximum number of epochs
  num_sanity_val_steps: 1  # sanity check before training starts
  fast_dev_run: False      # if True, runs one batch of each dataloader

hydra:
  job:
    chdir: True
  run:
    dir: experiments/${name}/${now:%Y-%m-%d}_${now:%H-%M-%S}